{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70695a2d-4891-428c-a80a-5866045a0da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------+---------+-------------------+\n|             country| ind|latitude|longitude|          timestamp|\n+--------------------+----+--------+---------+-------------------+\n|British Indian Oc...|3337|-65.2363|  21.9622|2020-06-29T05:02:22|\n|Antarctica (the t...|2418|-88.4642| -171.061|2022-05-27T11:30:59|\n|Antarctica (the t...|1434|-39.6186| -128.291|2020-03-06T21:00:23|\n|Antarctica (the t...|5162|-71.6607| -149.206|2019-09-27T19:06:43|\n|Antarctica (the t...|1335|-77.9931| -175.682|2022-03-19T17:29:42|\n+--------------------+----+--------+---------+-------------------+\nonly showing top 5 rows\n\n+---+-------------------+-----------+-----+---------+\n|age|        date_joined| first_name|  ind|last_name|\n+---+-------------------+-----------+-----+---------+\n| 27|2016-03-08T13:38:37|Christopher| 2015| Bradshaw|\n| 59|2017-05-12T21:22:17|  Alexander|10673|Cervantes|\n| 39|2016-06-29T20:43:59|  Christina| 6398|Davenport|\n| 20|2015-10-23T04:13:23| Alexandria| 3599| Alvarado|\n| 32|2015-12-18T05:07:36|  Alexander| 5623|Blanchard|\n+---+-------------------+-----------+-----+---------+\nonly showing top 5 rows\n\n+--------------+--------------------+----------+--------------+--------------------+-----+-----------------+--------------------+--------------------+--------------------+---------------------+--------------------+\n|      category|         description|downloaded|follower_count|           image_src|index|is_image_or_video|         poster_name|       save_location|            tag_list|                title|           unique_id|\n+--------------+--------------------+----------+--------------+--------------------+-----+-----------------+--------------------+--------------------+--------------------+---------------------+--------------------+\n|       tattoos|Image uploaded by...|         1|           15M|https://i.pinimg....| 8586|            image|         We Heart It|Local save in /da...|Mens Body Tattoos...| Pretty ♥ discover...|c338b1c8-7c6a-4a1...|\n|    home-decor|И хоть у шведов л...|         1|          136k|https://i.pinimg....| 6447|            image|PUFIK Interiors &...|Local save in /da...|Cheap Home Decor,...|〚 Warm natural to...|d3039535-5767-426...|\n|     christmas|Features: Materia...|         1|            5k|https://i.pinimg....| 1706|            image|            Wear24-7|Local save in /da...|Merry Christmas T...| Standing Figurine...|b5c8a1b5-9e90-452...|\n|     christmas|Christmas decorat...|         1|           46k|https://i.pinimg....| 2482|            video|Life on Summerhil...|Local save in /da...|Diy Christmas Dec...| FORNT PORCH CHRIS...|08604f20-fa17-4b9...|\n|event-planning|This fabulous DIY...|         1|          985k|https://i.pinimg....| 4585|            image|DIY Joy - Crafts,...|Local save in /da...|Cheap Favors,Wedd...| She Attaches Crys...|aa873546-701b-40d...|\n+--------------+--------------------+----------+--------------+--------------------+-----+-----------------+--------------------+--------------------+--------------------+---------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Import the credentials setup function\n",
    "%run ./aws_credentials\n",
    "\n",
    "set_aws_credentials(spark)\n",
    "\n",
    "# Then proceed to load your data\n",
    "df_geo = spark.read.json(\"s3a://kafka-bucket-osaze/topics/testbuck1.geo/partition=0/\")\n",
    "df_geo.show(5)\n",
    "\n",
    "df_user = spark.read.json(\"s3a://kafka-bucket-osaze/topics/testbuck1.user/partition=0/\")\n",
    "df_user.show(5)\n",
    "\n",
    "df_pin = spark.read.json(\"s3a://kafka-bucket-osaze/topics/testbuck1.pin/partition=0/\")\n",
    "df_pin.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3d73a4-01f6-4dc1-b82a-4fabed6e106e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------------------+--------------------+--------------+--------------------+--------------------+-----------------+--------------------+--------------+--------------+\n| ind|           unique_id|                title|         description|follower_count|         poster_name|            tag_list|is_image_or_video|           image_src| save_location|      category|\n+----+--------------------+---------------------+--------------------+--------------+--------------------+--------------------+-----------------+--------------------+--------------+--------------+\n|8586|c338b1c8-7c6a-4a1...| Pretty ♥ discover...|Image uploaded by...|      15000000|         We Heart It|Mens Body Tattoos...|            image|https://i.pinimg....|       tattoos|       tattoos|\n|6447|d3039535-5767-426...|〚 Warm natural to...|И хоть у шведов л...|          null|PUFIK Interiors &...|Cheap Home Decor,...|            image|https://i.pinimg....|    home-decor|    home-decor|\n|1706|b5c8a1b5-9e90-452...| Standing Figurine...|Features: Materia...|          null|            Wear24-7|Merry Christmas T...|            image|https://i.pinimg....|     christmas|     christmas|\n|2482|08604f20-fa17-4b9...| FORNT PORCH CHRIS...|Christmas decorat...|          null|Life on Summerhil...|Diy Christmas Dec...|            video|https://i.pinimg....|     christmas|     christmas|\n|4585|aa873546-701b-40d...| She Attaches Crys...|This fabulous DIY...|          null|DIY Joy - Crafts,...|Cheap Favors,Wedd...|            image|https://i.pinimg....|event-planning|event-planning|\n+----+--------------------+---------------------+--------------------+--------------+--------------------+--------------------+-----------------+--------------------+--------------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Replace empty strings and null-like values with None\n",
    "df_pin_cleaned = df_pin.replace(['', ' ', 'N/A', 'null', 'None'], None)\n",
    "\n",
    "# Fix follower_count to be numeric\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\n",
    "    \"follower_count\",\n",
    "    when(col(\"follower_count\").contains(\"K\"), regexp_replace(\"follower_count\", \"K\", \"e3\"))\n",
    "    .when(col(\"follower_count\").contains(\"M\"), regexp_replace(\"follower_count\", \"M\", \"e6\"))\n",
    "    .when(col(\"follower_count\").contains(\"B\"), regexp_replace(\"follower_count\", \"B\", \"e9\"))\n",
    "    .otherwise(col(\"follower_count\"))\n",
    ")\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\", col(\"follower_count\").cast(\"double\").cast(\"int\"))\n",
    "\n",
    "# Clean save_location\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"save_location\", regexp_replace(col(\"save_location\"), r\".*/\", \"\"))\n",
    "\n",
    "# Rename and reorder\n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "ordered_cols = [\n",
    "    \"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\",\n",
    "    \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"\n",
    "]\n",
    "df_pin_cleaned = df_pin_cleaned.select(*ordered_cols)\n",
    "\n",
    "# Preview\n",
    "df_pin_cleaned.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58747a55-6a38-47e5-adf8-ed971577bb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+-------------------+\n| ind|             country|         coordinates|          timestamp|\n+----+--------------------+--------------------+-------------------+\n|3337|British Indian Oc...| [-65.2363, 21.9622]|2020-06-29 05:02:22|\n|2418|Antarctica (the t...|[-88.4642, -171.061]|2022-05-27 11:30:59|\n|1434|Antarctica (the t...|[-39.6186, -128.291]|2020-03-06 21:00:23|\n|5162|Antarctica (the t...|[-71.6607, -149.206]|2019-09-27 19:06:43|\n|1335|Antarctica (the t...|[-77.9931, -175.682]|2022-03-19 17:29:42|\n+----+--------------------+--------------------+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, col, to_timestamp\n",
    "\n",
    "# Create coordinates column as an array of latitude and longitude\n",
    "df_geo_cleaned = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "# Drop latitude and longitude columns\n",
    "df_geo_cleaned = df_geo_cleaned.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Convert timestamp to proper timestamp format\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Reorder columns\n",
    "geo_cols_order = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "df_geo_cleaned = df_geo_cleaned.select(*geo_cols_order)\n",
    "\n",
    "# Show cleaned DataFrame\n",
    "df_geo_cleaned.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b092d5-3d92-4e6f-9ee1-9a5c761f093f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+-------------------+\n|  ind|           user_name|age|        date_joined|\n+-----+--------------------+---+-------------------+\n| 2015|Christopher Bradshaw| 27|2016-03-08 13:38:37|\n|10673| Alexander Cervantes| 59|2017-05-12 21:22:17|\n| 6398| Christina Davenport| 39|2016-06-29 20:43:59|\n| 3599| Alexandria Alvarado| 20|2015-10-23 04:13:23|\n| 5623| Alexander Blanchard| 32|2015-12-18 05:07:36|\n+-----+--------------------+---+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Create user_name column by combining first_name and last_name\n",
    "df_user_cleaned = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# Drop first_name and last_name columns\n",
    "df_user_cleaned = df_user_cleaned.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert date_joined column to timestamp\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "\n",
    "# Reorder columns\n",
    "user_cols_order = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "df_user_cleaned = df_user_cleaned.select(*user_cols_order)\n",
    "\n",
    "# Show cleaned DataFrame\n",
    "df_user_cleaned.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b332412-0f19-4b24-b031-db651aa80319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+--------------+--------------+\n|country                                     |category      |category_count|\n+--------------------------------------------+--------------+--------------+\n|Afghanistan                                 |education     |5             |\n|Albania                                     |art           |11            |\n|Algeria                                     |quotes        |7             |\n|American Samoa                              |travel        |4             |\n|Andorra                                     |tattoos       |4             |\n|Angola                                      |diy-and-crafts|1             |\n|Anguilla                                    |home-decor    |3             |\n|Antarctica (the territory South of 60 deg S)|beauty        |2             |\n|Antigua and Barbuda                         |art           |3             |\n|Argentina                                   |tattoos       |4             |\n|Armenia                                     |diy-and-crafts|10            |\n|Aruba                                       |mens-fashion  |10            |\n|Australia                                   |travel        |3             |\n|Austria                                     |travel        |4             |\n|Azerbaijan                                  |event-planning|4             |\n|Bahamas                                     |event-planning|2             |\n|Bahrain                                     |diy-and-crafts|1             |\n|Bangladesh                                  |christmas     |6             |\n|Barbados                                    |art           |5             |\n|Belarus                                     |travel        |1             |\n+--------------------------------------------+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Join the geo and pin cleaned DataFrames on 'ind'\n",
    "df_joined = df_pin_cleaned.join(df_geo_cleaned, on=\"ind\")\n",
    "\n",
    "# Step 2: Group by country and category, and count how many posts per category per country\n",
    "df_grouped = df_joined.groupBy(\"country\", \"category\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# Step 3: Use window function to rank categories by popularity within each country\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "df_ranked = df_grouped.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Step 4: Filter only the top-ranked (most popular) category per country\n",
    "df_result = df_ranked.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Show result\n",
    "df_result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a24df8-3525-4b99-8ed0-92d51745e38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+\n|post_year|category      |category_count|\n+---------+--------------+--------------+\n|2018     |travel        |12            |\n|2018     |quotes        |12            |\n|2018     |education     |11            |\n|2018     |diy-and-crafts|10            |\n|2018     |beauty        |10            |\n|2018     |art           |8             |\n|2018     |mens-fashion  |8             |\n|2018     |christmas     |7             |\n|2018     |home-decor    |5             |\n|2018     |tattoos       |5             |\n|2018     |vehicles      |5             |\n|2018     |event-planning|2             |\n|2018     |finance       |1             |\n|2019     |christmas     |23            |\n|2019     |diy-and-crafts|22            |\n|2019     |education     |13            |\n|2019     |mens-fashion  |12            |\n|2019     |art           |11            |\n|2019     |finance       |11            |\n|2019     |travel        |10            |\n+---------+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, col, count\n",
    "\n",
    "# Step 1: Join the cleaned pin and geo DataFrames\n",
    "df_joined = df_pin_cleaned.join(df_geo_cleaned, on=\"ind\")\n",
    "\n",
    "# Step 2: Extract the year from timestamp\n",
    "df_with_year = df_joined.withColumn(\"post_year\", year(col(\"timestamp\")))\n",
    "\n",
    "# Step 3: Filter for years between 2018 and 2022\n",
    "df_filtered = df_with_year.filter((col(\"post_year\") >= 2018) & (col(\"post_year\") <= 2022))\n",
    "\n",
    "# Step 4: Group by year and category, and count the number of posts\n",
    "df_category_year = df_filtered.groupBy(\"post_year\", \"category\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# Step 5: Order by post_year and category_count (optional)\n",
    "df_category_year.orderBy(\"post_year\", \"category_count\", ascending=[True, False]).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed234ed-fb57-40e0-9de0-497b111e7326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+----------------------------------------------------------------+--------------+\n|country                                     |poster_name                                                     |follower_count|\n+--------------------------------------------+----------------------------------------------------------------+--------------+\n|Afghanistan                                 |9GAG                                                            |3000000       |\n|Albania                                     |The Minds Journal                                               |5000000       |\n|Algeria                                     |Apartment Therapy                                               |5000000       |\n|American Samoa                              |Mamas Uncut                                                     |8000000       |\n|Andorra                                     |Teachers Pay Teachers                                           |1000000       |\n|Angola                                      |Grzegorz Makarewicz                                             |244           |\n|Anguilla                                    |We Heart It                                                     |15000000      |\n|Antarctica (the territory South of 60 deg S)|Refinery29                                                      |1000000       |\n|Antigua and Barbuda                         |Country Living Magazine                                         |1000000       |\n|Argentina                                   |Boop Decals                                                     |null          |\n|Armenia                                     |Michelle {CraftyMorning.com}                                    |null          |\n|Aruba                                       |Jacqueline MB Designs                                           |159           |\n|Australia                                   |Cultura Colectiva                                               |1000000       |\n|Austria                                     |Wanderlust Chloe ✈️ Travel guides, inspo and adventure travel ✈️|null          |\n|Azerbaijan                                  |Style Me Pretty                                                 |6000000       |\n|Bahamas                                     |Stylecraze                                                      |2000000       |\n|Bahrain                                     |Rainy Day Mum                                                   |null          |\n|Bangladesh                                  |Better Homes and Gardens                                        |4000000       |\n|Barbados                                    |Arty Crafty Kids                                                |null          |\n|Belarus                                     |Daily Mail                                                      |2000000       |\n+--------------------------------------------+----------------------------------------------------------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Step 1: Join pin and geo data\n",
    "df_joined = df_pin_cleaned.join(df_geo_cleaned, on=\"ind\")\n",
    "\n",
    "# Step 2: Create a window partitioned by country and ordered by follower_count descending\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(F.desc(\"follower_count\"))\n",
    "\n",
    "# Step 3: Rank users by follower count in each country\n",
    "df_ranked = df_joined.withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "\n",
    "# Step 4: Filter to get top user per country\n",
    "df_top_per_country = df_ranked.filter(col(\"rank\") == 1).select(\"country\", \"poster_name\", \"follower_count\")\n",
    "\n",
    "df_top_per_country.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a83a19-b091-497a-bd86-4092c1687e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n| country|follower_count|\n+--------+--------------+\n|Anguilla|      15000000|\n+--------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Use the result from previous query\n",
    "df_most_followed_user = df_top_per_country.orderBy(F.desc(\"follower_count\")).limit(1)\n",
    "\n",
    "df_most_followed_user.select(\"country\", \"follower_count\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f2a43a-e052-449d-ad31-03dc839d83ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n|age_group|category  |category_count|\n+---------+----------+--------------+\n|+50      |travel    |7             |\n|18-24    |art       |33            |\n|25-35    |christmas |21            |\n|36-50    |home-decor|12            |\n+---------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Join pin and user data on 'ind'\n",
    "df_joined_age = df_pin_cleaned.join(df_user_cleaned, on=\"ind\")\n",
    "\n",
    "# Step 2: Create age_group column\n",
    "df_age_grouped = df_joined_age.withColumn(\n",
    "    \"age_group\",\n",
    "    when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "    .otherwise(\"+50\")\n",
    ")\n",
    "\n",
    "# Step 3: Group and count categories per age group\n",
    "df_age_category_popular = (\n",
    "    df_age_grouped.groupBy(\"age_group\", \"category\")\n",
    "    .agg(count(\"*\").alias(\"category_count\"))\n",
    ")\n",
    "\n",
    "# Step 4: Rank categories within each age_group\n",
    "windowSpec = Window.partitionBy(\"age_group\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# Step 5: Filter to get the most popular category per age group\n",
    "top_category_per_age_group = df_age_category_popular.withColumn(\n",
    "    \"rank\", row_number().over(windowSpec)\n",
    ").filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Show result\n",
    "top_category_per_age_group.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15aaf3ff-6a5e-45e3-88d4-52cd5d3f05ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+\n|age_group|median_follower_count|\n+---------+---------------------+\n|36-50    |213.0                |\n|+50      |196.0                |\n|18-24    |1000000.0            |\n|25-35    |412.0                |\n+---------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Step 1: Join user and pin data\n",
    "df_joined_age = df_user_cleaned.join(df_pin_cleaned, on=\"ind\")\n",
    "\n",
    "# Step 2: Create age_group column\n",
    "df_age_grouped = df_joined_age.withColumn(\n",
    "    \"age_group\",\n",
    "    when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "    .otherwise(\"+50\")\n",
    ").select(\"age_group\", \"follower_count\")\n",
    "\n",
    "# Step 3: Collect median follower count per age group using approxQuantile\n",
    "age_groups = df_age_grouped.select(\"age_group\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Step 4: Loop through each group and get median\n",
    "results = []\n",
    "for group in age_groups:\n",
    "    median = df_age_grouped.filter(col(\"age_group\") == group)\\\n",
    "                           .approxQuantile(\"follower_count\", [0.5], 0.01)[0]\n",
    "    results.append((group, median))\n",
    "\n",
    "# Step 5: Create DataFrame from results\n",
    "median_schema = [\"age_group\", \"median_follower_count\"]\n",
    "df_median_followers = spark.createDataFrame(results, median_schema)\n",
    "\n",
    "# Display result\n",
    "df_median_followers.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df846b3-85ed-49b0-9a6a-3fd1726a8020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n|post_year|number_users_joined|\n+---------+-------------------+\n|     2015|                214|\n|     2016|                224|\n|     2017|                 87|\n+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, col, count\n",
    "\n",
    "# Step 1: Extract the year from the date_joined column\n",
    "df_joined_year = df_user_cleaned.withColumn(\"post_year\", year(col(\"date_joined\")))\n",
    "\n",
    "# Step 2: Filter years between 2015 and 2020 and group by year\n",
    "df_users_joined_filtered = df_joined_year.filter(\n",
    "    (col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020)\n",
    ").groupBy(\"post_year\").agg(count(\"*\").alias(\"number_users_joined\"))\n",
    "\n",
    "# Step 3: Display the result\n",
    "df_users_joined_filtered.orderBy(\"post_year\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea6bb4a-317f-4cd2-95b0-196141379ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+\n|post_year|median_follower_count|\n+---------+---------------------+\n|     2015|              2000000|\n|     2016|                  313|\n|     2017|                  213|\n+---------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, expr, col\n",
    "\n",
    "# Step 1: Join pin and user data on `ind`\n",
    "df_user_pin = df_user_cleaned.join(df_pin_cleaned, on=\"ind\")\n",
    "\n",
    "# Step 2: Extract post year\n",
    "df_with_year = df_user_pin.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "\n",
    "# Step 3: Filter users who joined between 2015 and 2020\n",
    "df_filtered = df_with_year.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Step 4: Compute median follower count\n",
    "df_median = df_filtered.groupBy(\"post_year\").agg(\n",
    "    expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")\n",
    ")\n",
    "\n",
    "# Step 5: Show result\n",
    "df_median.orderBy(\"post_year\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b886625-f72b-4c56-9abf-968d1a08e541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------------+\n|age_group|post_year|median_follower_count|\n+---------+---------+---------------------+\n|+50      |2015     |196                  |\n|+50      |2016     |189                  |\n|+50      |2017     |409                  |\n|18-24    |2015     |2000000              |\n|18-24    |2016     |49                   |\n|18-24    |2017     |612                  |\n|25-35    |2015     |null                 |\n|25-35    |2016     |437                  |\n|25-35    |2017     |26                   |\n|36-50    |2015     |0                    |\n|36-50    |2016     |399                  |\n|36-50    |2017     |213                  |\n+---------+---------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, expr, when\n",
    "\n",
    "# Step 1: Join user and pin DataFrames\n",
    "df_joined = df_user_cleaned.join(df_pin_cleaned, on=\"ind\")\n",
    "\n",
    "# Step 2: Extract year from date_joined\n",
    "df_with_year = df_joined.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "\n",
    "# Step 3: Create age_group column\n",
    "df_with_age_group = df_with_year.withColumn(\n",
    "    \"age_group\",\n",
    "    when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "    .otherwise(\"+50\")\n",
    ")\n",
    "\n",
    "# Step 4: Filter years between 2015 and 2020\n",
    "df_filtered = df_with_age_group.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Step 5: Compute median follower count grouped by age_group and post_year\n",
    "df_median_grouped = df_filtered.groupBy(\"age_group\", \"post_year\").agg(\n",
    "    expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")\n",
    ")\n",
    "\n",
    "# Step 6: Show results\n",
    "df_median_grouped.orderBy(\"age_group\", \"post_year\").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pinterest_note",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}